{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "hyperas.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tcglarry/useful_keras/blob/master/hyperas.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "p26COp4rR8S3",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# 調參工具  Hyperas\n",
        " \n",
        "## Dataset - Fashion-MNIST\n",
        "\n",
        "![alt text](https://cdn-images-1.medium.com/max/1000/1*QQVbuP2SEasB0XAmvjW0AA.jpeg)"
      ]
    },
    {
      "metadata": {
        "id": "z_-jXEC_VB8K",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Code"
      ]
    },
    {
      "metadata": {
        "id": "jGb_LcJl0R9Q",
        "colab_type": "code",
        "outputId": "89312d2d-0cc8-408c-b663-53e18913f141",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "cell_type": "code",
      "source": [
        "# sudo pip3 install --ignore-installed --upgrade tensorflow\n",
        "from __future__ import print_function\n",
        "import tensorflow as tf\n",
        "import keras.backend.tensorflow_backend as KTF\n",
        "config = tf.ConfigProto()\n",
        "config.gpu_options.allow_growth = True\n",
        "session = tf.Session(config=config)\n",
        "KTF.set_session(session)\n",
        "import keras\n",
        "#import tensorflow as tf\n",
        "print(keras.__version__)\n",
        "print(tf.__version__)\n",
        "# To ignore keep_dims warning\n",
        "tf.logging.set_verbosity(tf.logging.ERROR)\n",
        "\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "2.2.4\n",
            "1.12.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "0MGXn9L80l2C",
        "colab_type": "code",
        "outputId": "c602ba99-8628-46b0-d74a-439245798629",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1512
        }
      },
      "cell_type": "code",
      "source": [
        "# 安裝套件\n",
        "\n",
        "!pip install hyperas\n",
        "!pip install hyperopt"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting hyperas\n",
            "  Downloading https://files.pythonhosted.org/packages/54/72/5533b6bf9b47dc33685c3e62c391d6eab5785a648a5ffa841e240a3db3fe/hyperas-0.4.tar.gz\n",
            "Requirement already satisfied: keras in /usr/local/lib/python3.6/dist-packages (from hyperas) (2.2.4)\n",
            "Collecting hyperopt (from hyperas)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ce/9f/f6324af3fc43f352e568b5850695c30ed7dd14af06a94f97953ff9187569/hyperopt-0.1.1-py3-none-any.whl (117kB)\n",
            "\u001b[K    100% |████████████████████████████████| 122kB 11.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: entrypoints in /usr/local/lib/python3.6/dist-packages (from hyperas) (0.2.3)\n",
            "Collecting jupyter (from hyperas)\n",
            "  Downloading https://files.pythonhosted.org/packages/83/df/0f5dd132200728a86190397e1ea87cd76244e42d39ec5e88efd25b2abd7e/jupyter-1.0.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: nbformat in /usr/local/lib/python3.6/dist-packages (from hyperas) (4.4.0)\n",
            "Requirement already satisfied: nbconvert in /usr/local/lib/python3.6/dist-packages (from hyperas) (5.4.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from keras->hyperas) (1.0.5)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from keras->hyperas) (1.1.0)\n",
            "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.6/dist-packages (from keras->hyperas) (1.14.6)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from keras->hyperas) (1.0.6)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from keras->hyperas) (3.13)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras->hyperas) (2.8.0)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from keras->hyperas) (1.11.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from hyperopt->hyperas) (0.16.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.6/dist-packages (from hyperopt->hyperas) (2.2)\n",
            "Collecting pymongo (from hyperopt->hyperas)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b1/45/5440555b901a8416196fbf2499c4678ef74de8080c007104107a8cfdda20/pymongo-3.7.2-cp36-cp36m-manylinux1_x86_64.whl (408kB)\n",
            "\u001b[K    100% |████████████████████████████████| 409kB 20.3MB/s \n",
            "\u001b[?25hCollecting qtconsole (from jupyter->hyperas)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e0/7a/8aefbc0ed078dec7951ac9a06dcd1869243ecd7bcbce26fa47bf5e469a8f/qtconsole-4.4.3-py2.py3-none-any.whl (113kB)\n",
            "\u001b[K    100% |████████████████████████████████| 122kB 32.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: notebook in /usr/local/lib/python3.6/dist-packages (from jupyter->hyperas) (5.2.2)\n",
            "Collecting ipywidgets (from jupyter->hyperas)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/30/9a/a008c7b1183fac9e52066d80a379b3c64eab535bd9d86cdc29a0b766fd82/ipywidgets-7.4.2-py2.py3-none-any.whl (111kB)\n",
            "\u001b[K    100% |████████████████████████████████| 112kB 28.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: ipykernel in /usr/local/lib/python3.6/dist-packages (from jupyter->hyperas) (4.6.1)\n",
            "Collecting jupyter-console (from jupyter->hyperas)\n",
            "  Downloading https://files.pythonhosted.org/packages/cb/ee/6374ae8c21b7d0847f9c3722dcdfac986b8e54fa9ad9ea66e1eb6320d2b8/jupyter_console-6.0.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: jsonschema!=2.5.0,>=2.4 in /usr/local/lib/python3.6/dist-packages (from nbformat->hyperas) (2.6.0)\n",
            "Requirement already satisfied: jupyter-core in /usr/local/lib/python3.6/dist-packages (from nbformat->hyperas) (4.4.0)\n",
            "Requirement already satisfied: traitlets>=4.1 in /usr/local/lib/python3.6/dist-packages (from nbformat->hyperas) (4.3.2)\n",
            "Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.6/dist-packages (from nbformat->hyperas) (0.2.0)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.6/dist-packages (from nbconvert->hyperas) (3.0.2)\n",
            "Requirement already satisfied: testpath in /usr/local/lib/python3.6/dist-packages (from nbconvert->hyperas) (0.4.2)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.6/dist-packages (from nbconvert->hyperas) (2.1.3)\n",
            "Requirement already satisfied: mistune>=0.8.1 in /usr/local/lib/python3.6/dist-packages (from nbconvert->hyperas) (0.8.4)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.6/dist-packages (from nbconvert->hyperas) (1.4.2)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.6/dist-packages (from nbconvert->hyperas) (0.5.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.6/dist-packages (from nbconvert->hyperas) (2.10)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.6/dist-packages (from networkx->hyperopt->hyperas) (4.3.0)\n",
            "Requirement already satisfied: jupyter-client>=4.1 in /usr/local/lib/python3.6/dist-packages (from qtconsole->jupyter->hyperas) (5.2.3)\n",
            "Requirement already satisfied: terminado>=0.3.3; sys_platform != \"win32\" in /usr/local/lib/python3.6/dist-packages (from notebook->jupyter->hyperas) (0.8.1)\n",
            "Requirement already satisfied: tornado>=4 in /usr/local/lib/python3.6/dist-packages (from notebook->jupyter->hyperas) (4.5.3)\n",
            "Requirement already satisfied: ipython>=4.0.0; python_version >= \"3.3\" in /usr/local/lib/python3.6/dist-packages (from ipywidgets->jupyter->hyperas) (5.5.0)\n",
            "Collecting widgetsnbextension~=3.4.0 (from ipywidgets->jupyter->hyperas)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/8a/81/35789a3952afb48238289171728072d26d6e76649ddc8b3588657a2d78c1/widgetsnbextension-3.4.2-py2.py3-none-any.whl (2.2MB)\n",
            "\u001b[K    100% |████████████████████████████████| 2.2MB 11.8MB/s \n",
            "\u001b[?25hCollecting prompt-toolkit<2.1.0,>=2.0.0 (from jupyter-console->jupyter->hyperas)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d1/e6/adb3be5576f5d27c6faa33f1e9fea8fe5dbd9351db12148de948507e352c/prompt_toolkit-2.0.7-py3-none-any.whl (338kB)\n",
            "\u001b[K    100% |████████████████████████████████| 348kB 19.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: webencodings in /usr/local/lib/python3.6/dist-packages (from bleach->nbconvert->hyperas) (0.5.1)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.6/dist-packages (from jinja2->nbconvert->hyperas) (1.1.0)\n",
            "Requirement already satisfied: pyzmq>=13 in /usr/local/lib/python3.6/dist-packages (from jupyter-client>=4.1->qtconsole->jupyter->hyperas) (17.0.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from jupyter-client>=4.1->qtconsole->jupyter->hyperas) (2.5.3)\n",
            "Requirement already satisfied: ptyprocess; os_name != \"nt\" in /usr/local/lib/python3.6/dist-packages (from terminado>=0.3.3; sys_platform != \"win32\"->notebook->jupyter->hyperas) (0.6.0)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.6/dist-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets->jupyter->hyperas) (0.7.5)\n",
            "Requirement already satisfied: pexpect; sys_platform != \"win32\" in /usr/local/lib/python3.6/dist-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets->jupyter->hyperas) (4.6.0)\n",
            "Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.6/dist-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets->jupyter->hyperas) (0.8.1)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.6/dist-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets->jupyter->hyperas) (40.6.2)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.6/dist-packages (from prompt-toolkit<2.1.0,>=2.0.0->jupyter-console->jupyter->hyperas) (0.1.7)\n",
            "Building wheels for collected packages: hyperas\n",
            "  Running setup.py bdist_wheel for hyperas ... \u001b[?25l-\b \bdone\n",
            "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/06/38/3f/27826f57fae60ef788ceb47e2c649590ab8af31f42075325d2\n",
            "Successfully built hyperas\n",
            "\u001b[31mipython 5.5.0 has requirement prompt-toolkit<2.0.0,>=1.0.4, but you'll have prompt-toolkit 2.0.7 which is incompatible.\u001b[0m\n",
            "Installing collected packages: pymongo, hyperopt, qtconsole, widgetsnbextension, ipywidgets, prompt-toolkit, jupyter-console, jupyter, hyperas\n",
            "  Found existing installation: prompt-toolkit 1.0.15\n",
            "    Uninstalling prompt-toolkit-1.0.15:\n",
            "      Successfully uninstalled prompt-toolkit-1.0.15\n",
            "Successfully installed hyperas-0.4 hyperopt-0.1.1 ipywidgets-7.4.2 jupyter-1.0.0 jupyter-console-6.0.0 prompt-toolkit-2.0.7 pymongo-3.7.2 qtconsole-4.4.3 widgetsnbextension-3.4.2\n",
            "Requirement already satisfied: hyperopt in /usr/local/lib/python3.6/dist-packages (0.1.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from hyperopt) (1.1.0)\n",
            "Requirement already satisfied: pymongo in /usr/local/lib/python3.6/dist-packages (from hyperopt) (3.7.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from hyperopt) (1.11.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from hyperopt) (0.16.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.6/dist-packages (from hyperopt) (2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from hyperopt) (1.14.6)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.6/dist-packages (from networkx->hyperopt) (4.3.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "krm_n0O06dj0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "! rm -rf hyperas.ipynb"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "CvAPxwCn0t0Z",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# See: https://stackoverflow.com/questions/49920031/get-the-path-of-the-notebook-on-google-colab\n",
        "# Install the PyDrive wrapper & import libraries.\n",
        "!pip install -U -q PyDrive\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "# Authenticate and create the PyDrive client.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "\n",
        "# Copy/download the file\n",
        "fid = drive.ListFile({'q':\"title='hyperas.ipynb'\"}).GetList()[0]['id']\n",
        "f = drive.CreateFile({'id': fid})\n",
        "f.GetContentFile('hyperas.ipynb')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "gf1hIg5u05kf",
        "colab_type": "code",
        "outputId": "39c58902-5e29-4f08-9fec-f8b9ca364c88",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        }
      },
      "cell_type": "code",
      "source": [
        "! ls -al"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total 116\n",
            "drwxr-xr-x 1 root root  4096 Dec  9 06:45 .\n",
            "drwxr-xr-x 1 root root  4096 Dec  9 06:42 ..\n",
            "-rw-r--r-- 1 root root  2520 Dec  9 06:45 adc.json\n",
            "drwxr-xr-x 1 root root  4096 Dec  9 06:45 .config\n",
            "-rw-r--r-- 1 root root 87868 Dec  9 06:45 hyperas.ipynb\n",
            "drwxr-xr-x 2 root root  4096 Dec  5 17:39 sample_data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "4wHKwRKmpC6P",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Best model:\n",
        "{'Activation': 0, \n",
        "'Activation_1': 0, 'Dense': 2, 'Dense_1': 2, 'Dense_2': 3, 'Dropout': 0.020862912321193482, 'Dropout_1': 0.0910332772894481, 'Dropout_2': 0.8618283661109736, 'batch_size': 0, 'choiceval': 2, 'conditional': 0, 'lr': 0, 'lr_1': 1, 'lr_2': 0}\n"
      ]
    },
    {
      "metadata": {
        "id": "Faq1HLl81KsJ",
        "colab_type": "code",
        "outputId": "82af0e06-090a-46d9-a82f-8337b4ff39f2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 12952
        }
      },
      "cell_type": "code",
      "source": [
        "\n",
        "from keras.datasets import mnist\n",
        "from keras.layers.core import Dense, Dropout, Activation\n",
        "from keras.models import Sequential\n",
        "\n",
        "\n",
        "from hyperas import optim\n",
        "from hyperas.distributions import choice, uniform, conditional\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.utils import np_utils\n",
        "from hyperopt import Trials, STATUS_OK, tpe\n",
        "  \n",
        "from keras import backend as K\n",
        "\n",
        "import seaborn as sns\n",
        "\n",
        "\n",
        "\n",
        "X_test_global = None\n",
        "Y_test_global = None\n",
        "\n",
        "\n",
        "def data():\n",
        "  \n",
        " \n",
        "  from keras.datasets import fashion_mnist\n",
        "  from keras.datasets import mnist\n",
        "  from sklearn.model_selection import train_test_split\n",
        "\n",
        "  from keras.layers.core import Dense, Dropout, Activation\n",
        "  from keras.models import Sequential\n",
        "\n",
        "\n",
        "  from hyperas import optim\n",
        "  from hyperas.distributions import choice, uniform, conditional\n",
        "  from keras import backend as K\n",
        "\n",
        "  import seaborn as sns\n",
        "\n",
        "  sns.set()\n",
        "  \n",
        "  print('Data function')\n",
        "  img_rows, img_cols = 28, 28\n",
        "  nb_classes = 10\n",
        "  (X_train, y_train), (X_test, y_test) = fashion_mnist.load_data()\n",
        "\n",
        "  X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2,random_state=12345)\n",
        "\n",
        "  pltsize=1\n",
        "  \n",
        "  print (X_train.shape)\n",
        "  \n",
        "  Y_train = np_utils.to_categorical(y_train, nb_classes)\n",
        "  Y_val = np_utils.to_categorical(y_val, nb_classes)\n",
        "  Y_test= np_utils.to_categorical(y_test, nb_classes)\n",
        "  \n",
        "  \n",
        "  plt.figure(figsize=(10*pltsize, pltsize))\n",
        "\n",
        "  for i in range(10):\n",
        "\n",
        "    plt.subplot(1,10,i+1)\n",
        "    plt.axis('off')\n",
        "    plt.imshow(X_train[i,:,:], cmap=\"gray\")\n",
        "    plt.title('Class: '+str(y_train[i]))\n",
        "    print('Training sample',i,': class:',y_train[i], ', one-hot encoded:', Y_train[i])\n",
        "\n",
        "\n",
        "  X_train = X_train.reshape(X_train.shape[0], 784)\n",
        "  X_val = X_val.reshape(X_val.shape[0], 784)\n",
        "  X_test = X_test.reshape(X_test.shape[0], 784)\n",
        "  X_train = X_train.astype('float32')\n",
        "  X_val = X_val.astype('float32')\n",
        "  X_test = X_test.astype('float32')\n",
        "  X_train /= 255\n",
        "  X_val /= 255\n",
        "  X_test/= 255\n",
        "  \n",
        "  global X_test_global\n",
        "  X_test_global = X_test\n",
        "  global Y_test_global\n",
        "  Y_test_global= Y_test\n",
        "  \n",
        "  print (Y_test_global.shape)\n",
        "  \n",
        "  return X_train, Y_train, X_val, Y_val\n",
        "\n",
        "\n",
        "\n",
        "def create_model(X_train, Y_train, X_val, Y_val):\n",
        "    \"\"\"\n",
        "    Model providing function:\n",
        "\n",
        "    Create Keras model with double curly brackets dropped-in as needed.\n",
        "    Return value has to be a valid python dictionary with two customary keys:\n",
        "        - loss: Specify a numeric evaluation metric to be minimized\n",
        "        - status: Just use STATUS_OK and see hyperopt documentation if not feasible\n",
        "    The last one is optional, though recommended, namely:\n",
        "        - model: specify the model just created so that we can later use it again.\n",
        "    \n",
        "    {'Activation': 0, \n",
        "    'Activation_1': 0, \n",
        "    'Dense': 2, 'Dense_1': 2, 'Dense_2': 3, \n",
        "    'Dropout': 0.020862912321193482, 'Dropout_1': 0.0910332772894481, 'Dropout_2': 0.8618283661109736, \n",
        "    'batch_size': 0, \n",
        "    'choiceval': 2, 'conditional': 0, 'lr': 0, 'lr_1': 1, 'lr_2': 0}\n",
        "    \"\"\"\n",
        "    model = Sequential()\n",
        "    #layer 1\n",
        "    model.add(Dense({{choice([128, 256, 512, 1024])}}, input_shape=(784,)))\n",
        "    model.add(Activation('relu'))\n",
        "    model.add(Dropout({{uniform(0, 1)}}))\n",
        "    \n",
        "    # layer 2\n",
        "    model.add(Dense({{choice([128, 256, 512, 1024])}}))\n",
        "    model.add(Activation({{choice(['relu', 'sigmoid'])}}))\n",
        "    model.add(Dropout({{uniform(0, 1)}}))\n",
        "    \n",
        "    # layer 3\n",
        "    if conditional({{choice(['two', 'three'])}}) == 'three':\n",
        "        model.add(Dense({{choice([128, 256, 512, 1024])}}))\n",
        "        model.add(Activation({{choice(['relu', 'sigmoid'])}}))\n",
        "        model.add(Dropout({{uniform(0, 1)}}))\n",
        "    \n",
        "    model.add(Dense(10))\n",
        "    model.add(Activation('softmax'))\n",
        "    \n",
        "    \n",
        "    adam = keras.optimizers.Adam(lr={{choice([10**-3, 10**-4])}})\n",
        "    rmsprop = keras.optimizers.RMSprop(lr={{choice([10**-3, 10**-4])}})\n",
        "    sgd = keras.optimizers.SGD(lr={{choice([10**-3, 10**-4])}})\n",
        "   \n",
        "    choiceval = {{choice(['adam', 'sgd', 'rmsprop'])}}\n",
        "    if choiceval == 'adam':\n",
        "        optim = adam\n",
        "    elif choiceval == 'rmsprop':\n",
        "        optim = rmsprop\n",
        "    else:\n",
        "        optim = sgd\n",
        "        \n",
        "\n",
        "    model.compile(loss='categorical_crossentropy', metrics=['accuracy'],\n",
        "                  optimizer=optim)\n",
        "\n",
        "    model.fit(X_train, Y_train,\n",
        "              batch_size={{choice([32, 64, 128])}},\n",
        "              epochs=5,\n",
        "              verbose=2,\n",
        "              validation_data=(X_val, Y_val))\n",
        "    score, acc = model.evaluate(X_val, Y_val, verbose=0)\n",
        "    print('Test accuracy:', acc)\n",
        "    \n",
        "    return {'loss': -acc, 'status': STATUS_OK, 'model': model}\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    notebook_name='hyperas'\n",
        "    \n",
        "    X_train, Y_train, X_val, Y_val = data()\n",
        "    print ('Val set')\n",
        "    print (X_val.shape)\n",
        "    print (Y_val.shape)\n",
        "    best_run, best_model = optim.minimize(model=create_model,\n",
        "                                          data=data,\n",
        "                                          algo=tpe.suggest,\n",
        "                                          max_evals=30,\n",
        "                                          trials=Trials(),\n",
        "                                          notebook_name=notebook_name)\n",
        "    print (best_model.summary())\n",
        "    print(\"Evalutation of best performing model:\")\n",
        "    print(best_model.evaluate(  X_test_global, Y_test_global))\n",
        "    print(\"Best performing model chosen hyper-parameters:\")\n",
        "    print(best_run)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Data function\n",
            "Downloading data from http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\n",
            "32768/29515 [=================================] - 0s 1us/step\n",
            "Downloading data from http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\n",
            "26427392/26421880 [==============================] - 0s 0us/step\n",
            "Downloading data from http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\n",
            "8192/5148 [===============================================] - 0s 0us/step\n",
            "Downloading data from http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\n",
            "4423680/4422102 [==============================] - 0s 0us/step\n",
            "(48000, 28, 28)\n",
            "Training sample 0 : class: 3 , one-hot encoded: [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
            "Training sample 1 : class: 6 , one-hot encoded: [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
            "Training sample 2 : class: 4 , one-hot encoded: [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
            "Training sample 3 : class: 7 , one-hot encoded: [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
            "Training sample 4 : class: 4 , one-hot encoded: [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
            "Training sample 5 : class: 9 , one-hot encoded: [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
            "Training sample 6 : class: 0 , one-hot encoded: [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "Training sample 7 : class: 9 , one-hot encoded: [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
            "Training sample 8 : class: 0 , one-hot encoded: [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "Training sample 9 : class: 3 , one-hot encoded: [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
            "(10000, 10)\n",
            "Val set\n",
            "(12000, 784)\n",
            "(12000, 10)\n",
            ">>> Imports:\n",
            "#coding=utf-8\n",
            "\n",
            "from __future__ import print_function\n",
            "\n",
            "try:\n",
            "    import tensorflow as tf\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    import keras.backend.tensorflow_backend as KTF\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    import keras\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    from pydrive.auth import GoogleAuth\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    from pydrive.drive import GoogleDrive\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    from google.colab import auth\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    from oauth2client.client import GoogleCredentials\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    from keras.datasets import mnist\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    from keras.layers.core import Dense, Dropout, Activation\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    from keras.models import Sequential\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    from hyperas import optim\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    from hyperas.distributions import choice, uniform, conditional\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    import matplotlib.pyplot as plt\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    from keras.utils import np_utils\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    from hyperopt import Trials, STATUS_OK, tpe\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    from keras import backend as K\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    import seaborn as sns\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    from keras.datasets import fashion_mnist\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    from keras.datasets import mnist\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    from sklearn.model_selection import train_test_split\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    from keras.layers.core import Dense, Dropout, Activation\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    from keras.models import Sequential\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    from hyperas import optim\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    from hyperas.distributions import choice, uniform, conditional\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    from keras import backend as K\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    import seaborn as sns\n",
            "except:\n",
            "    pass\n",
            "\n",
            ">>> Hyperas search space:\n",
            "\n",
            "def get_space():\n",
            "    return {\n",
            "        'Dense': hp.choice('Dense', [128, 256, 512, 1024]),\n",
            "        'Dropout': hp.uniform('Dropout', 0, 1),\n",
            "        'Dense_1': hp.choice('Dense_1', [128, 256, 512, 1024]),\n",
            "        'Activation': hp.choice('Activation', ['relu', 'sigmoid']),\n",
            "        'Dropout_1': hp.uniform('Dropout_1', 0, 1),\n",
            "        'conditional': hp.choice('conditional', ['two', 'three']),\n",
            "        'Dense_2': hp.choice('Dense_2', [128, 256, 512, 1024]),\n",
            "        'Activation_1': hp.choice('Activation_1', ['relu', 'sigmoid']),\n",
            "        'Dropout_2': hp.uniform('Dropout_2', 0, 1),\n",
            "        'lr': hp.choice('lr', [10**-3, 10**-4]),\n",
            "        'lr_1': hp.choice('lr_1', [10**-3, 10**-4]),\n",
            "        'lr_2': hp.choice('lr_2', [10**-3, 10**-4]),\n",
            "        'choiceval': hp.choice('choiceval', ['adam', 'sgd', 'rmsprop']),\n",
            "        'batch_size': hp.choice('batch_size', [32, 64, 128]),\n",
            "    }\n",
            "\n",
            ">>> Data\n",
            "   1: \n",
            "   2: \n",
            "   3: \n",
            "   4: from keras.datasets import fashion_mnist\n",
            "   5: from keras.datasets import mnist\n",
            "   6: from sklearn.model_selection import train_test_split\n",
            "   7: \n",
            "   8: from keras.layers.core import Dense, Dropout, Activation\n",
            "   9: from keras.models import Sequential\n",
            "  10: \n",
            "  11: \n",
            "  12: from hyperas import optim\n",
            "  13: from hyperas.distributions import choice, uniform, conditional\n",
            "  14: from keras import backend as K\n",
            "  15: \n",
            "  16: import seaborn as sns\n",
            "  17: \n",
            "  18: sns.set()\n",
            "  19: \n",
            "  20: print('Data function')\n",
            "  21: img_rows, img_cols = 28, 28\n",
            "  22: nb_classes = 10\n",
            "  23: (X_train, y_train), (X_test, y_test) = fashion_mnist.load_data()\n",
            "  24: \n",
            "  25: X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2,random_state=12345)\n",
            "  26: \n",
            "  27: pltsize=1\n",
            "  28: \n",
            "  29: print (X_train.shape)\n",
            "  30: \n",
            "  31: Y_train = np_utils.to_categorical(y_train, nb_classes)\n",
            "  32: Y_val = np_utils.to_categorical(y_val, nb_classes)\n",
            "  33: Y_test= np_utils.to_categorical(y_test, nb_classes)\n",
            "  34: \n",
            "  35: \n",
            "  36: plt.figure(figsize=(10*pltsize, pltsize))\n",
            "  37: \n",
            "  38: for i in range(10):\n",
            "  39: \n",
            "  40:   plt.subplot(1,10,i+1)\n",
            "  41:   plt.axis('off')\n",
            "  42:   plt.imshow(X_train[i,:,:], cmap=\"gray\")\n",
            "  43:   plt.title('Class: '+str(y_train[i]))\n",
            "  44:   print('Training sample',i,': class:',y_train[i], ', one-hot encoded:', Y_train[i])\n",
            "  45: \n",
            "  46: \n",
            "  47: X_train = X_train.reshape(X_train.shape[0], 784)\n",
            "  48: X_val = X_val.reshape(X_val.shape[0], 784)\n",
            "  49: X_test = X_test.reshape(X_test.shape[0], 784)\n",
            "  50: X_train = X_train.astype('float32')\n",
            "  51: X_val = X_val.astype('float32')\n",
            "  52: X_test = X_test.astype('float32')\n",
            "  53: X_train /= 255\n",
            "  54: X_val /= 255\n",
            "  55: X_test/= 255\n",
            "  56: \n",
            "  57: global X_test_global\n",
            "  58: X_test_global = X_test\n",
            "  59: global Y_test_global\n",
            "  60: Y_test_global= Y_test\n",
            "  61: \n",
            "  62: print (Y_test_global.shape)\n",
            "  63: \n",
            "  64: \n",
            "  65: \n",
            "  66: \n",
            ">>> Resulting replaced keras model:\n",
            "\n",
            "   1: def keras_fmin_fnct(space):\n",
            "   2: \n",
            "   3:     \"\"\"\n",
            "   4:     Model providing function:\n",
            "   5: \n",
            "   6:     Create Keras model with double curly brackets dropped-in as needed.\n",
            "   7:     Return value has to be a valid python dictionary with two customary keys:\n",
            "   8:         - loss: Specify a numeric evaluation metric to be minimized\n",
            "   9:         - status: Just use STATUS_OK and see hyperopt documentation if not feasible\n",
            "  10:     The last one is optional, though recommended, namely:\n",
            "  11:         - model: specify the model just created so that we can later use it again.\n",
            "  12:     \n",
            "  13:     {'Activation': 0, \n",
            "  14:     'Activation_1': 0, \n",
            "  15:     'Dense': 2, 'Dense_1': 2, 'Dense_2': 3, \n",
            "  16:     'Dropout': 0.020862912321193482, 'Dropout_1': 0.0910332772894481, 'Dropout_2': 0.8618283661109736, \n",
            "  17:     'batch_size': 0, \n",
            "  18:     'choiceval': 2, 'conditional': 0, 'lr': 0, 'lr_1': 1, 'lr_2': 0}\n",
            "  19:     \"\"\"\n",
            "  20:     model = Sequential()\n",
            "  21:     #layer 1\n",
            "  22:     model.add(Dense(space['Dense'], input_shape=(784,)))\n",
            "  23:     model.add(Activation('relu'))\n",
            "  24:     model.add(Dropout(space['Dropout']))\n",
            "  25:     \n",
            "  26:     # layer 2\n",
            "  27:     model.add(Dense(space['Dense_1']))\n",
            "  28:     model.add(Activation(space['Activation']))\n",
            "  29:     model.add(Dropout(space['Dropout_1']))\n",
            "  30:     \n",
            "  31:     # layer 3\n",
            "  32:     if conditional(space['conditional']) == 'three':\n",
            "  33:         model.add(Dense(space['Dense_2']))\n",
            "  34:         model.add(Activation(space['Activation_1']))\n",
            "  35:         model.add(Dropout(space['Dropout_2']))\n",
            "  36:     \n",
            "  37:     model.add(Dense(10))\n",
            "  38:     model.add(Activation('softmax'))\n",
            "  39:     \n",
            "  40:     \n",
            "  41:     adam = keras.optimizers.Adam(lr=space['lr'])\n",
            "  42:     rmsprop = keras.optimizers.RMSprop(lr=space['lr_1'])\n",
            "  43:     sgd = keras.optimizers.SGD(lr=space['lr_2'])\n",
            "  44:    \n",
            "  45:     choiceval = space['choiceval']\n",
            "  46:     if choiceval == 'adam':\n",
            "  47:         optim = adam\n",
            "  48:     elif choiceval == 'rmsprop':\n",
            "  49:         optim = rmsprop\n",
            "  50:     else:\n",
            "  51:         optim = sgd\n",
            "  52:         \n",
            "  53: \n",
            "  54:     model.compile(loss='categorical_crossentropy', metrics=['accuracy'],\n",
            "  55:                   optimizer=optim)\n",
            "  56: \n",
            "  57:     model.fit(X_train, Y_train,\n",
            "  58:               batch_size=space['batch_size'],\n",
            "  59:               epochs=5,\n",
            "  60:               verbose=2,\n",
            "  61:               validation_data=(X_val, Y_val))\n",
            "  62:     score, acc = model.evaluate(X_val, Y_val, verbose=0)\n",
            "  63:     print('Test accuracy:', acc)\n",
            "  64:     \n",
            "  65:     return {'loss': -acc, 'status': STATUS_OK, 'model': model}\n",
            "  66: \n",
            "Data function\n",
            "(48000, 28, 28)\n",
            "Training sample 0 : class: 3 , one-hot encoded: [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
            "Training sample 1 : class: 6 , one-hot encoded: [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
            "Training sample 2 : class: 4 , one-hot encoded: [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
            "Training sample 3 : class: 7 , one-hot encoded: [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
            "Training sample 4 : class: 4 , one-hot encoded: [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
            "Training sample 5 : class: 9 , one-hot encoded: [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
            "Training sample 6 : class: 0 , one-hot encoded: [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "Training sample 7 : class: 9 , one-hot encoded: [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
            "Training sample 8 : class: 0 , one-hot encoded: [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "Training sample 9 : class: 3 , one-hot encoded: [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
            "(10000, 10)\n",
            "Train on 48000 samples, validate on 12000 samples\n",
            "Epoch 1/5\n",
            " - 5s - loss: 1.6436 - acc: 0.5149 - val_loss: 1.2209 - val_acc: 0.6673\n",
            "Epoch 2/5\n",
            " - 4s - loss: 1.1030 - acc: 0.6570 - val_loss: 0.9481 - val_acc: 0.6890\n",
            "Epoch 3/5\n",
            " - 4s - loss: 0.9286 - acc: 0.6885 - val_loss: 0.8345 - val_acc: 0.7190\n",
            "Epoch 4/5\n",
            " - 4s - loss: 0.8433 - acc: 0.7155 - val_loss: 0.7661 - val_acc: 0.7510\n",
            "Epoch 5/5\n",
            " - 4s - loss: 0.7849 - acc: 0.7382 - val_loss: 0.7215 - val_acc: 0.7663\n",
            "Test accuracy: 0.7663333333333333\n",
            "Train on 48000 samples, validate on 12000 samples\n",
            "Epoch 1/5\n",
            " - 5s - loss: 1.5871 - acc: 0.3877 - val_loss: 0.8426 - val_acc: 0.6456\n",
            "Epoch 2/5\n",
            " - 4s - loss: 1.3524 - acc: 0.4736 - val_loss: 0.9386 - val_acc: 0.5972\n",
            "Epoch 3/5\n",
            " - 4s - loss: 1.3016 - acc: 0.4923 - val_loss: 0.8109 - val_acc: 0.6876\n",
            "Epoch 4/5\n",
            " - 4s - loss: 1.2711 - acc: 0.5091 - val_loss: 0.9587 - val_acc: 0.5551\n",
            "Epoch 5/5\n",
            " - 4s - loss: 1.2490 - acc: 0.5183 - val_loss: 0.8669 - val_acc: 0.6440\n",
            "Test accuracy: 0.644\n",
            "Train on 48000 samples, validate on 12000 samples\n",
            "Epoch 1/5\n",
            " - 2s - loss: 2.9194 - acc: 0.1018 - val_loss: 2.3829 - val_acc: 0.0972\n",
            "Epoch 2/5\n",
            " - 2s - loss: 2.8660 - acc: 0.1038 - val_loss: 2.3420 - val_acc: 0.0973\n",
            "Epoch 3/5\n",
            " - 2s - loss: 2.8194 - acc: 0.1065 - val_loss: 2.3089 - val_acc: 0.0989\n",
            "Epoch 4/5\n",
            " - 2s - loss: 2.7861 - acc: 0.1088 - val_loss: 2.2816 - val_acc: 0.1078\n",
            "Epoch 5/5\n",
            " - 2s - loss: 2.7522 - acc: 0.1089 - val_loss: 2.2583 - val_acc: 0.1428\n",
            "Test accuracy: 0.14283333333333334\n",
            "Train on 48000 samples, validate on 12000 samples\n",
            "Epoch 1/5\n",
            " - 2s - loss: 3.4584 - acc: 0.1000 - val_loss: 2.3724 - val_acc: 0.1003\n",
            "Epoch 2/5\n",
            " - 2s - loss: 3.4155 - acc: 0.0990 - val_loss: 2.3377 - val_acc: 0.1003\n",
            "Epoch 3/5\n",
            " - 2s - loss: 3.3954 - acc: 0.0990 - val_loss: 2.3212 - val_acc: 0.1304\n",
            "Epoch 4/5\n",
            " - 2s - loss: 3.3715 - acc: 0.1000 - val_loss: 2.3137 - val_acc: 0.0981\n",
            "Epoch 5/5\n",
            " - 2s - loss: 3.3597 - acc: 0.1006 - val_loss: 2.3109 - val_acc: 0.0978\n",
            "Test accuracy: 0.09783333333333333\n",
            "Train on 48000 samples, validate on 12000 samples\n",
            "Epoch 1/5\n",
            " - 5s - loss: 2.1398 - acc: 0.2250 - val_loss: 1.4900 - val_acc: 0.5098\n",
            "Epoch 2/5\n",
            " - 4s - loss: 1.7403 - acc: 0.3191 - val_loss: 1.2224 - val_acc: 0.5373\n",
            "Epoch 3/5\n",
            " - 4s - loss: 1.6147 - acc: 0.3624 - val_loss: 1.1268 - val_acc: 0.5701\n",
            "Epoch 4/5\n",
            " - 4s - loss: 1.5577 - acc: 0.3889 - val_loss: 1.0772 - val_acc: 0.5532\n",
            "Epoch 5/5\n",
            " - 4s - loss: 1.5364 - acc: 0.4020 - val_loss: 1.0849 - val_acc: 0.5493\n",
            "Test accuracy: 0.54925\n",
            "Train on 48000 samples, validate on 12000 samples\n",
            "Epoch 1/5\n",
            " - 9s - loss: 0.7883 - acc: 0.7350 - val_loss: 0.4721 - val_acc: 0.8445\n",
            "Epoch 2/5\n",
            " - 8s - loss: 0.5077 - acc: 0.8249 - val_loss: 0.4024 - val_acc: 0.8601\n",
            "Epoch 3/5\n",
            " - 8s - loss: 0.4440 - acc: 0.8448 - val_loss: 0.3659 - val_acc: 0.8742\n",
            "Epoch 4/5\n",
            " - 8s - loss: 0.4074 - acc: 0.8573 - val_loss: 0.3470 - val_acc: 0.8770\n",
            "Epoch 5/5\n",
            " - 8s - loss: 0.3862 - acc: 0.8641 - val_loss: 0.3432 - val_acc: 0.8795\n",
            "Test accuracy: 0.8795\n",
            "Train on 48000 samples, validate on 12000 samples\n",
            "Epoch 1/5\n",
            " - 4s - loss: 2.2722 - acc: 0.1281 - val_loss: 1.7373 - val_acc: 0.2026\n",
            "Epoch 2/5\n",
            " - 3s - loss: 1.9926 - acc: 0.1785 - val_loss: 1.7311 - val_acc: 0.2019\n",
            "Epoch 3/5\n",
            " - 3s - loss: 1.9218 - acc: 0.1917 - val_loss: 1.8140 - val_acc: 0.2135\n",
            "Epoch 4/5\n",
            " - 3s - loss: 1.8839 - acc: 0.2059 - val_loss: 1.8622 - val_acc: 0.2134\n",
            "Epoch 5/5\n",
            " - 3s - loss: 1.8617 - acc: 0.2116 - val_loss: 2.0035 - val_acc: 0.2139\n",
            "Test accuracy: 0.21391666666666667\n",
            "Train on 48000 samples, validate on 12000 samples\n",
            "Epoch 1/5\n",
            " - 3s - loss: 1.8557 - acc: 0.3076 - val_loss: 1.0644 - val_acc: 0.6212\n",
            "Epoch 2/5\n",
            " - 2s - loss: 1.0603 - acc: 0.5884 - val_loss: 0.7531 - val_acc: 0.7311\n",
            "Epoch 3/5\n",
            " - 2s - loss: 0.8372 - acc: 0.6812 - val_loss: 0.6477 - val_acc: 0.7447\n",
            "Epoch 4/5\n",
            " - 2s - loss: 0.7384 - acc: 0.7229 - val_loss: 0.5907 - val_acc: 0.7710\n",
            "Epoch 5/5\n",
            " - 2s - loss: 0.6863 - acc: 0.7436 - val_loss: 0.5594 - val_acc: 0.7903\n",
            "Test accuracy: 0.7903333333333333\n",
            "Train on 48000 samples, validate on 12000 samples\n",
            "Epoch 1/5\n",
            " - 3s - loss: 1.8042 - acc: 0.3255 - val_loss: 1.0658 - val_acc: 0.6442\n",
            "Epoch 2/5\n",
            " - 2s - loss: 1.2255 - acc: 0.5211 - val_loss: 0.8286 - val_acc: 0.6740\n",
            "Epoch 3/5\n",
            " - 2s - loss: 1.0364 - acc: 0.5964 - val_loss: 0.7246 - val_acc: 0.7192\n",
            "Epoch 4/5\n",
            " - 2s - loss: 0.9418 - acc: 0.6355 - val_loss: 0.6611 - val_acc: 0.7578\n",
            "Epoch 5/5\n",
            " - 2s - loss: 0.8706 - acc: 0.6687 - val_loss: 0.6157 - val_acc: 0.7856\n",
            "Test accuracy: 0.7855833333333333\n",
            "Train on 48000 samples, validate on 12000 samples\n",
            "Epoch 1/5\n",
            " - 9s - loss: 2.5709 - acc: 0.0995 - val_loss: 2.3056 - val_acc: 0.0983\n",
            "Epoch 2/5\n",
            " - 8s - loss: 2.4512 - acc: 0.1033 - val_loss: 2.2998 - val_acc: 0.0977\n",
            "Epoch 3/5\n",
            " - 8s - loss: 2.4060 - acc: 0.1058 - val_loss: 2.2901 - val_acc: 0.0973\n",
            "Epoch 4/5\n",
            " - 8s - loss: 2.3689 - acc: 0.1110 - val_loss: 2.2868 - val_acc: 0.1073\n",
            "Epoch 5/5\n",
            " - 8s - loss: 2.3484 - acc: 0.1114 - val_loss: 2.2875 - val_acc: 0.1792\n",
            "Test accuracy: 0.17925\n",
            "Train on 48000 samples, validate on 12000 samples\n",
            "Epoch 1/5\n",
            " - 3s - loss: 1.8805 - acc: 0.3296 - val_loss: 1.2834 - val_acc: 0.5896\n",
            "Epoch 2/5\n",
            " - 2s - loss: 1.1918 - acc: 0.5665 - val_loss: 0.8994 - val_acc: 0.6817\n",
            "Epoch 3/5\n",
            " - 2s - loss: 0.9320 - acc: 0.6486 - val_loss: 0.7472 - val_acc: 0.7192\n",
            "Epoch 4/5\n",
            " - 2s - loss: 0.8080 - acc: 0.6978 - val_loss: 0.6536 - val_acc: 0.7578\n",
            "Epoch 5/5\n",
            " - 2s - loss: 0.7236 - acc: 0.7334 - val_loss: 0.5898 - val_acc: 0.7813\n",
            "Test accuracy: 0.7813333333333333\n",
            "Train on 48000 samples, validate on 12000 samples\n",
            "Epoch 1/5\n",
            " - 3s - loss: 2.4066 - acc: 0.0973 - val_loss: 2.2901 - val_acc: 0.0943\n",
            "Epoch 2/5\n",
            " - 2s - loss: 2.2961 - acc: 0.1386 - val_loss: 2.2011 - val_acc: 0.2507\n",
            "Epoch 3/5\n",
            " - 2s - loss: 2.2209 - acc: 0.1783 - val_loss: 2.1315 - val_acc: 0.3399\n",
            "Epoch 4/5\n",
            " - 2s - loss: 2.1565 - acc: 0.2217 - val_loss: 2.0713 - val_acc: 0.4081\n",
            "Epoch 5/5\n",
            " - 2s - loss: 2.1033 - acc: 0.2568 - val_loss: 2.0167 - val_acc: 0.4683\n",
            "Test accuracy: 0.46825\n",
            "Train on 48000 samples, validate on 12000 samples\n",
            "Epoch 1/5\n",
            " - 4s - loss: 1.4630 - acc: 0.4724 - val_loss: 0.7919 - val_acc: 0.6821\n",
            "Epoch 2/5\n",
            " - 3s - loss: 0.7959 - acc: 0.7022 - val_loss: 0.6013 - val_acc: 0.7698\n",
            "Epoch 3/5\n",
            " - 3s - loss: 0.6575 - acc: 0.7575 - val_loss: 0.5284 - val_acc: 0.8024\n",
            "Epoch 4/5\n",
            " - 3s - loss: 0.5929 - acc: 0.7819 - val_loss: 0.4906 - val_acc: 0.8199\n",
            "Epoch 5/5\n",
            " - 3s - loss: 0.5523 - acc: 0.8004 - val_loss: 0.4650 - val_acc: 0.8302\n",
            "Test accuracy: 0.8301666666666667\n",
            "Train on 48000 samples, validate on 12000 samples\n",
            "Epoch 1/5\n",
            " - 5s - loss: 0.8262 - acc: 0.7088 - val_loss: 0.5017 - val_acc: 0.8278\n",
            "Epoch 2/5\n",
            " - 3s - loss: 0.5512 - acc: 0.8024 - val_loss: 0.4367 - val_acc: 0.8469\n",
            "Epoch 3/5\n",
            " - 3s - loss: 0.4921 - acc: 0.8224 - val_loss: 0.3948 - val_acc: 0.8619\n",
            "Epoch 4/5\n",
            " - 3s - loss: 0.4633 - acc: 0.8319 - val_loss: 0.3752 - val_acc: 0.8685\n",
            "Epoch 5/5\n",
            " - 3s - loss: 0.4382 - acc: 0.8419 - val_loss: 0.3608 - val_acc: 0.8753\n",
            "Test accuracy: 0.8753333333333333\n",
            "Train on 48000 samples, validate on 12000 samples\n",
            "Epoch 1/5\n",
            " - 5s - loss: 2.5983 - acc: 0.1011 - val_loss: 2.3355 - val_acc: 0.1013\n",
            "Epoch 2/5\n",
            " - 5s - loss: 2.5365 - acc: 0.1036 - val_loss: 2.2967 - val_acc: 0.1134\n",
            "Epoch 3/5\n",
            " - 5s - loss: 2.5163 - acc: 0.1042 - val_loss: 2.2768 - val_acc: 0.1378\n",
            "Epoch 4/5\n",
            " - 5s - loss: 2.4966 - acc: 0.1079 - val_loss: 2.2639 - val_acc: 0.1681\n",
            "Epoch 5/5\n",
            " - 5s - loss: 2.4843 - acc: 0.1102 - val_loss: 2.2536 - val_acc: 0.2224\n",
            "Test accuracy: 0.22241666666666668\n",
            "Train on 48000 samples, validate on 12000 samples\n",
            "Epoch 1/5\n",
            " - 8s - loss: 0.7039 - acc: 0.7433 - val_loss: 0.4587 - val_acc: 0.8357\n",
            "Epoch 2/5\n",
            " - 7s - loss: 0.5427 - acc: 0.8037 - val_loss: 0.4211 - val_acc: 0.8421\n",
            "Epoch 3/5\n",
            " - 7s - loss: 0.5023 - acc: 0.8170 - val_loss: 0.3937 - val_acc: 0.8562\n",
            "Epoch 4/5\n",
            " - 7s - loss: 0.4799 - acc: 0.8249 - val_loss: 0.3851 - val_acc: 0.8638\n",
            "Epoch 5/5\n",
            " - 7s - loss: 0.4662 - acc: 0.8305 - val_loss: 0.3735 - val_acc: 0.8632\n",
            "Test accuracy: 0.8631666666666666\n",
            "Train on 48000 samples, validate on 12000 samples\n",
            "Epoch 1/5\n",
            " - 10s - loss: 2.3792 - acc: 0.1026 - val_loss: 2.3034 - val_acc: 0.1618\n",
            "Epoch 2/5\n",
            " - 9s - loss: 2.3366 - acc: 0.1098 - val_loss: 2.2836 - val_acc: 0.1700\n",
            "Epoch 3/5\n",
            " - 9s - loss: 2.3148 - acc: 0.1151 - val_loss: 2.2669 - val_acc: 0.1943\n",
            "Epoch 4/5\n",
            " - 9s - loss: 2.2971 - acc: 0.1221 - val_loss: 2.2508 - val_acc: 0.2369\n",
            "Epoch 5/5\n",
            " - 8s - loss: 2.2797 - acc: 0.1335 - val_loss: 2.2346 - val_acc: 0.2954\n",
            "Test accuracy: 0.29541666666666666\n",
            "Train on 48000 samples, validate on 12000 samples\n",
            "Epoch 1/5\n",
            " - 4s - loss: 2.4266 - acc: 0.1183 - val_loss: 2.2893 - val_acc: 0.1374\n",
            "Epoch 2/5\n",
            " - 2s - loss: 2.3493 - acc: 0.1338 - val_loss: 2.2199 - val_acc: 0.2050\n",
            "Epoch 3/5\n",
            " - 2s - loss: 2.2863 - acc: 0.1539 - val_loss: 2.1627 - val_acc: 0.3213\n",
            "Epoch 4/5\n",
            " - 2s - loss: 2.2320 - acc: 0.1784 - val_loss: 2.1131 - val_acc: 0.4220\n",
            "Epoch 5/5\n",
            " - 2s - loss: 2.1864 - acc: 0.1994 - val_loss: 2.0682 - val_acc: 0.5047\n",
            "Test accuracy: 0.5046666666666667\n",
            "Train on 48000 samples, validate on 12000 samples\n",
            "Epoch 1/5\n",
            " - 3s - loss: 2.4539 - acc: 0.1199 - val_loss: 2.1923 - val_acc: 0.3607\n",
            "Epoch 2/5\n",
            " - 2s - loss: 2.2442 - acc: 0.1559 - val_loss: 2.1341 - val_acc: 0.4912\n",
            "Epoch 3/5\n",
            " - 2s - loss: 2.1822 - acc: 0.1886 - val_loss: 2.0657 - val_acc: 0.5522\n",
            "Epoch 4/5\n",
            " - 2s - loss: 2.1225 - acc: 0.2164 - val_loss: 1.9890 - val_acc: 0.5803\n",
            "Epoch 5/5\n",
            " - 2s - loss: 2.0686 - acc: 0.2400 - val_loss: 1.9102 - val_acc: 0.5983\n",
            "Test accuracy: 0.5983333333333334\n",
            "Train on 48000 samples, validate on 12000 samples\n",
            "Epoch 1/5\n",
            " - 4s - loss: 2.3665 - acc: 0.1142 - val_loss: 2.2060 - val_acc: 0.2442\n",
            "Epoch 2/5\n",
            " - 3s - loss: 2.3030 - acc: 0.1384 - val_loss: 2.1415 - val_acc: 0.3883\n",
            "Epoch 3/5\n",
            " - 3s - loss: 2.2428 - acc: 0.1659 - val_loss: 2.0830 - val_acc: 0.4501\n",
            "Epoch 4/5\n",
            " - 3s - loss: 2.1974 - acc: 0.1865 - val_loss: 2.0286 - val_acc: 0.4985\n",
            "Epoch 5/5\n",
            " - 3s - loss: 2.1480 - acc: 0.2127 - val_loss: 1.9771 - val_acc: 0.5410\n",
            "Test accuracy: 0.541\n",
            "Train on 48000 samples, validate on 12000 samples\n",
            "Epoch 1/5\n",
            " - 10s - loss: 0.5935 - acc: 0.7989 - val_loss: 0.4212 - val_acc: 0.8529\n",
            "Epoch 2/5\n",
            " - 9s - loss: 0.4206 - acc: 0.8494 - val_loss: 0.3688 - val_acc: 0.8716\n",
            "Epoch 3/5\n",
            " - 9s - loss: 0.3749 - acc: 0.8647 - val_loss: 0.3477 - val_acc: 0.8792\n",
            "Epoch 4/5\n",
            " - 9s - loss: 0.3492 - acc: 0.8759 - val_loss: 0.3216 - val_acc: 0.8870\n",
            "Epoch 5/5\n",
            " - 9s - loss: 0.3298 - acc: 0.8815 - val_loss: 0.3101 - val_acc: 0.8898\n",
            "Test accuracy: 0.8898333333333334\n",
            "Train on 48000 samples, validate on 12000 samples\n",
            "Epoch 1/5\n",
            " - 10s - loss: 0.6175 - acc: 0.7883 - val_loss: 0.4216 - val_acc: 0.8562\n",
            "Epoch 2/5\n",
            " - 9s - loss: 0.4233 - acc: 0.8488 - val_loss: 0.3667 - val_acc: 0.8718\n",
            "Epoch 3/5\n",
            " - 9s - loss: 0.3770 - acc: 0.8647 - val_loss: 0.3498 - val_acc: 0.8758\n",
            "Epoch 4/5\n",
            " - 9s - loss: 0.3501 - acc: 0.8742 - val_loss: 0.3288 - val_acc: 0.8855\n",
            "Epoch 5/5\n",
            " - 9s - loss: 0.3309 - acc: 0.8794 - val_loss: 0.3277 - val_acc: 0.8858\n",
            "Test accuracy: 0.8858333333333334\n",
            "Train on 48000 samples, validate on 12000 samples\n",
            "Epoch 1/5\n",
            " - 12s - loss: 0.5183 - acc: 0.8119 - val_loss: 0.4128 - val_acc: 0.8522\n",
            "Epoch 2/5\n",
            " - 10s - loss: 0.3940 - acc: 0.8533 - val_loss: 0.3609 - val_acc: 0.8670\n",
            "Epoch 3/5\n",
            " - 10s - loss: 0.3596 - acc: 0.8669 - val_loss: 0.3525 - val_acc: 0.8717\n",
            "Epoch 4/5\n",
            " - 10s - loss: 0.3367 - acc: 0.8738 - val_loss: 0.3286 - val_acc: 0.8833\n",
            "Epoch 5/5\n",
            " - 10s - loss: 0.3204 - acc: 0.8814 - val_loss: 0.3041 - val_acc: 0.8910\n",
            "Test accuracy: 0.891\n",
            "Train on 48000 samples, validate on 12000 samples\n",
            "Epoch 1/5\n",
            " - 12s - loss: 0.5433 - acc: 0.8031 - val_loss: 0.4114 - val_acc: 0.8453\n",
            "Epoch 2/5\n",
            " - 10s - loss: 0.4223 - acc: 0.8442 - val_loss: 0.3589 - val_acc: 0.8692\n",
            "Epoch 3/5\n",
            " - 10s - loss: 0.3862 - acc: 0.8563 - val_loss: 0.3433 - val_acc: 0.8826\n",
            "Epoch 4/5\n",
            " - 10s - loss: 0.3699 - acc: 0.8626 - val_loss: 0.3237 - val_acc: 0.8798\n",
            "Epoch 5/5\n",
            " - 10s - loss: 0.3488 - acc: 0.8694 - val_loss: 0.3217 - val_acc: 0.8831\n",
            "Test accuracy: 0.8830833333333333\n",
            "Train on 48000 samples, validate on 12000 samples\n",
            "Epoch 1/5\n",
            " - 12s - loss: 0.4930 - acc: 0.8213 - val_loss: 0.3686 - val_acc: 0.8667\n",
            "Epoch 2/5\n",
            " - 10s - loss: 0.3765 - acc: 0.8622 - val_loss: 0.3495 - val_acc: 0.8690\n",
            "Epoch 3/5\n",
            " - 10s - loss: 0.3359 - acc: 0.8751 - val_loss: 0.3381 - val_acc: 0.8750\n",
            "Epoch 4/5\n",
            " - 10s - loss: 0.3142 - acc: 0.8834 - val_loss: 0.2948 - val_acc: 0.8944\n",
            "Epoch 5/5\n",
            " - 10s - loss: 0.2946 - acc: 0.8898 - val_loss: 0.3189 - val_acc: 0.8868\n",
            "Test accuracy: 0.88675\n",
            "Train on 48000 samples, validate on 12000 samples\n",
            "Epoch 1/5\n",
            " - 12s - loss: 0.5156 - acc: 0.8118 - val_loss: 0.4086 - val_acc: 0.8498\n",
            "Epoch 2/5\n",
            " - 10s - loss: 0.3997 - acc: 0.8522 - val_loss: 0.3334 - val_acc: 0.8818\n",
            "Epoch 3/5\n",
            " - 10s - loss: 0.3651 - acc: 0.8649 - val_loss: 0.3185 - val_acc: 0.8841\n",
            "Epoch 4/5\n",
            " - 10s - loss: 0.3420 - acc: 0.8726 - val_loss: 0.3139 - val_acc: 0.8850\n",
            "Epoch 5/5\n",
            " - 10s - loss: 0.3211 - acc: 0.8795 - val_loss: 0.3195 - val_acc: 0.8869\n",
            "Test accuracy: 0.8869166666666667\n",
            "Train on 48000 samples, validate on 12000 samples\n",
            "Epoch 1/5\n",
            " - 12s - loss: 0.5756 - acc: 0.7893 - val_loss: 0.4100 - val_acc: 0.8519\n",
            "Epoch 2/5\n",
            " - 10s - loss: 0.4512 - acc: 0.8339 - val_loss: 0.3777 - val_acc: 0.8616\n",
            "Epoch 3/5\n",
            " - 10s - loss: 0.4177 - acc: 0.8452 - val_loss: 0.3560 - val_acc: 0.8782\n",
            "Epoch 4/5\n",
            " - 10s - loss: 0.3948 - acc: 0.8540 - val_loss: 0.3385 - val_acc: 0.8827\n",
            "Epoch 5/5\n",
            " - 10s - loss: 0.3845 - acc: 0.8575 - val_loss: 0.3290 - val_acc: 0.8756\n",
            "Test accuracy: 0.8755833333333334\n",
            "Train on 48000 samples, validate on 12000 samples\n",
            "Epoch 1/5\n",
            " - 12s - loss: 0.5188 - acc: 0.8108 - val_loss: 0.3857 - val_acc: 0.8655\n",
            "Epoch 2/5\n",
            " - 10s - loss: 0.3965 - acc: 0.8525 - val_loss: 0.3480 - val_acc: 0.8749\n",
            "Epoch 3/5\n",
            " - 10s - loss: 0.3618 - acc: 0.8667 - val_loss: 0.3350 - val_acc: 0.8787\n",
            "Epoch 4/5\n",
            " - 10s - loss: 0.3383 - acc: 0.8759 - val_loss: 0.3257 - val_acc: 0.8849\n",
            "Epoch 5/5\n",
            " - 10s - loss: 0.3200 - acc: 0.8808 - val_loss: 0.2977 - val_acc: 0.8925\n",
            "Test accuracy: 0.8925\n",
            "Train on 48000 samples, validate on 12000 samples\n",
            "Epoch 1/5\n",
            " - 12s - loss: 0.5090 - acc: 0.8139 - val_loss: 0.3780 - val_acc: 0.8634\n",
            "Epoch 2/5\n",
            " - 10s - loss: 0.3924 - acc: 0.8558 - val_loss: 0.3530 - val_acc: 0.8710\n",
            "Epoch 3/5\n",
            " - 10s - loss: 0.3550 - acc: 0.8694 - val_loss: 0.3362 - val_acc: 0.8788\n",
            "Epoch 4/5\n",
            " - 10s - loss: 0.3364 - acc: 0.8754 - val_loss: 0.3594 - val_acc: 0.8679\n",
            "Epoch 5/5\n",
            " - 10s - loss: 0.3180 - acc: 0.8823 - val_loss: 0.3055 - val_acc: 0.8906\n",
            "Test accuracy: 0.8905833333333333\n",
            "Train on 48000 samples, validate on 12000 samples\n",
            "Epoch 1/5\n",
            " - 13s - loss: 0.4930 - acc: 0.8218 - val_loss: 0.3667 - val_acc: 0.8669\n",
            "Epoch 2/5\n",
            " - 10s - loss: 0.3747 - acc: 0.8618 - val_loss: 0.3901 - val_acc: 0.8602\n",
            "Epoch 3/5\n",
            " - 10s - loss: 0.3415 - acc: 0.8732 - val_loss: 0.3401 - val_acc: 0.8732\n",
            "Epoch 4/5\n",
            " - 10s - loss: 0.3104 - acc: 0.8850 - val_loss: 0.3270 - val_acc: 0.8824\n",
            "Epoch 5/5\n",
            " - 10s - loss: 0.2943 - acc: 0.8914 - val_loss: 0.3197 - val_acc: 0.8852\n",
            "Test accuracy: 0.88525\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_93 (Dense)             (None, 512)               401920    \n",
            "_________________________________________________________________\n",
            "activation_93 (Activation)   (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dropout_66 (Dropout)         (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_94 (Dense)             (None, 512)               262656    \n",
            "_________________________________________________________________\n",
            "activation_94 (Activation)   (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dropout_67 (Dropout)         (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_95 (Dense)             (None, 10)                5130      \n",
            "_________________________________________________________________\n",
            "activation_95 (Activation)   (None, 10)                0         \n",
            "=================================================================\n",
            "Total params: 669,706\n",
            "Trainable params: 669,706\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Evalutation of best performing model:\n",
            "10000/10000 [==============================] - 1s 71us/step\n",
            "[0.3476714300394058, 0.8756]\n",
            "Best performing model chosen hyper-parameters:\n",
            "{'Activation': 0, 'Activation_1': 0, 'Dense': 2, 'Dense_1': 2, 'Dense_2': 3, 'Dropout': 0.09355930343068841, 'Dropout_1': 0.29593150317891614, 'Dropout_2': 0.6587292940008921, 'batch_size': 0, 'choiceval': 0, 'conditional': 0, 'lr': 0, 'lr_1': 1, 'lr_2': 0}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAlAAAABdCAYAAABq41iyAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nBSMEXelZB2YAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x7f6ab2bc7c18>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAlAAAABdCAYAAABq41iyAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nBSMEXelZB2YAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x7f6ab2b51e80>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "x5hBTHaG21Oa",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!py3clean ."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "UrXFIVZT278R",
        "colab_type": "code",
        "outputId": "e648d072-5ed2-4f9f-f55b-f1772395ba59",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "! cat temp_model.py"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cat: temp_model.py: No such file or directory\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}